#!/usr/bin/env python3
"""
Mem0Evomem æ€§èƒ½åŸºæº–æ¸¬è©¦

æ¸¬è©¦é …ç›®:
- BGE-M3 å–®æ–‡æœ¬åµŒå…¥å»¶é² (P50, P95, P99)
- BGE-M3 æ‰¹æ¬¡åµŒå…¥ååé‡ (10/100/1000 texts)
- ChromaDB æŸ¥è©¢æ€§èƒ½
- è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§
- Docker å®¹å™¨è³‡æºä½¿ç”¨

Author: EvoMem Team
License: Apache 2.0
"""

import time
import statistics
import psutil
import json
import sys
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from mem0.configs.embeddings.base import BaseEmbedderConfig
from mem0.embeddings.bge_m3 import BGEM3Embedding


class PerformanceBenchmark:
    """æ€§èƒ½åŸºæº–æ¸¬è©¦å·¥å…·"""

    def __init__(self):
        """åˆå§‹åŒ–æ¸¬è©¦ç’°å¢ƒ"""
        self.results: Dict[str, Any] = {
            "timestamp": datetime.now().isoformat(),
            "system_info": self._get_system_info(),
            "tests": {}
        }

        # åˆå§‹åŒ– BGE-M3 Embedder
        print("ğŸ”§ åˆå§‹åŒ– BGE-M3 Embedder...")
        config = BaseEmbedderConfig(
            model="BAAI/bge-m3",
            model_kwargs={
                "use_fp16": True,
                "device": "cpu",
                "max_length": 8192
            }
        )
        self.embedder = BGEM3Embedding(config)
        print("âœ… BGE-M3 Embedder è¼‰å…¥æˆåŠŸ\n")

    def _get_system_info(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±è³‡è¨Š"""
        return {
            "cpu_count": psutil.cpu_count(logical=True),
            "cpu_freq": psutil.cpu_freq().current if psutil.cpu_freq() else None,
            "memory_total_gb": round(psutil.virtual_memory().total / (1024**3), 2),
            "memory_available_gb": round(psutil.virtual_memory().available / (1024**3), 2),
            "python_version": f"{psutil.PROCFS_PATH}",
        }

    def _measure_memory_usage(self) -> Dict[str, float]:
        """æ¸¬é‡ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨"""
        process = psutil.Process()
        memory_info = process.memory_info()
        return {
            "rss_mb": round(memory_info.rss / (1024**2), 2),
            "vms_mb": round(memory_info.vms / (1024**2), 2),
            "percent": round(process.memory_percent(), 2)
        }

    def test_single_text_embedding_latency(self, iterations: int = 100) -> Dict[str, Any]:
        """
        æ¸¬è©¦å–®æ–‡æœ¬åµŒå…¥å»¶é²

        Args:
            iterations: æ¸¬è©¦æ¬¡æ•¸ (é»˜èª 100)

        Returns:
            æ¸¬è©¦çµæœ (P50, P95, P99, mean, std)
        """
        print(f"ğŸ“Š æ¸¬è©¦ 1/5: å–®æ–‡æœ¬åµŒå…¥å»¶é² ({iterations} æ¬¡è¿­ä»£)")

        test_texts = [
            "äººå·¥æ™ºæ…§æ­£åœ¨æ”¹è®Šä¸–ç•Œ",
            "æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹é‡è¦åˆ†æ”¯",
            "æ·±åº¦å­¸ç¿’åœ¨åœ–åƒè­˜åˆ¥é ˜åŸŸå–å¾—äº†å·¨å¤§çªç ´",
            "è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“æ—¥ç›Šæˆç†Ÿ",
            "å‘é‡æ•¸æ“šåº«æ˜¯ AI æ‡‰ç”¨çš„é‡è¦åŸºç¤è¨­æ–½"
        ]

        latencies = []
        memory_before = self._measure_memory_usage()

        for i in range(iterations):
            text = test_texts[i % len(test_texts)]
            start_time = time.perf_counter()
            _ = self.embedder.embed(text)
            end_time = time.perf_counter()
            latency_ms = (end_time - start_time) * 1000
            latencies.append(latency_ms)

            if (i + 1) % 20 == 0:
                print(f"  é€²åº¦: {i + 1}/{iterations}")

        memory_after = self._measure_memory_usage()

        # è¨ˆç®—çµ±è¨ˆæŒ‡æ¨™
        latencies_sorted = sorted(latencies)
        result = {
            "iterations": iterations,
            "mean_ms": round(statistics.mean(latencies), 2),
            "median_ms": round(statistics.median(latencies), 2),
            "std_ms": round(statistics.stdev(latencies), 2),
            "p50_ms": round(latencies_sorted[int(len(latencies_sorted) * 0.50)], 2),
            "p95_ms": round(latencies_sorted[int(len(latencies_sorted) * 0.95)], 2),
            "p99_ms": round(latencies_sorted[int(len(latencies_sorted) * 0.99)], 2),
            "min_ms": round(min(latencies), 2),
            "max_ms": round(max(latencies), 2),
            "memory_before_mb": memory_before["rss_mb"],
            "memory_after_mb": memory_after["rss_mb"],
            "memory_delta_mb": round(memory_after["rss_mb"] - memory_before["rss_mb"], 2)
        }

        print(f"  âœ… å®Œæˆ: P50={result['p50_ms']}ms, P95={result['p95_ms']}ms, P99={result['p99_ms']}ms\n")
        return result

    def test_batch_embedding_throughput(self, batch_sizes: List[int] = [10, 100, 1000]) -> Dict[str, Any]:
        """
        æ¸¬è©¦æ‰¹æ¬¡åµŒå…¥ååé‡

        Args:
            batch_sizes: æ‰¹æ¬¡å¤§å°åˆ—è¡¨ (é»˜èª [10, 100, 1000])

        Returns:
            æ¸¬è©¦çµæœ (æ¯å€‹æ‰¹æ¬¡å¤§å°çš„ååé‡)
        """
        print(f"ğŸ“Š æ¸¬è©¦ 2/5: æ‰¹æ¬¡åµŒå…¥ååé‡ (æ‰¹æ¬¡å¤§å°: {batch_sizes})")

        base_text = "é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡æœ¬ï¼Œç”¨æ–¼è©•ä¼°æ‰¹æ¬¡åµŒå…¥çš„æ€§èƒ½ã€‚"
        results = {}

        for batch_size in batch_sizes:
            print(f"  æ¸¬è©¦æ‰¹æ¬¡å¤§å°: {batch_size}")
            texts = [f"{base_text} ç·¨è™Ÿ: {i}" for i in range(batch_size)]

            memory_before = self._measure_memory_usage()
            start_time = time.perf_counter()
            _ = self.embedder.batch_embed(texts, batch_size=256)
            end_time = time.perf_counter()
            memory_after = self._measure_memory_usage()

            elapsed_s = end_time - start_time
            throughput = batch_size / elapsed_s

            results[f"batch_{batch_size}"] = {
                "batch_size": batch_size,
                "elapsed_s": round(elapsed_s, 2),
                "throughput_texts_per_s": round(throughput, 2),
                "latency_per_text_ms": round((elapsed_s / batch_size) * 1000, 2),
                "memory_before_mb": memory_before["rss_mb"],
                "memory_after_mb": memory_after["rss_mb"],
                "memory_delta_mb": round(memory_after["rss_mb"] - memory_before["rss_mb"], 2)
            }

            print(f"    âœ… ååé‡: {results[f'batch_{batch_size}']['throughput_texts_per_s']} texts/s")

        print()
        return results

    def test_embedding_quality(self) -> Dict[str, Any]:
        """
        æ¸¬è©¦åµŒå…¥è³ªé‡ (èªç¾©ç›¸ä¼¼åº¦)

        Returns:
            æ¸¬è©¦çµæœ (ç›¸ä¼¼æ–‡æœ¬è·é›¢ã€ä¸ç›¸ä¼¼æ–‡æœ¬è·é›¢)
        """
        print("ğŸ“Š æ¸¬è©¦ 3/5: åµŒå…¥è³ªé‡ (èªç¾©ç›¸ä¼¼åº¦)")

        # ç›¸ä¼¼æ–‡æœ¬å°
        similar_pairs = [
            ("äººå·¥æ™ºæ…§æŠ€è¡“ç™¼å±•è¿…é€Ÿ", "AI ç§‘æŠ€é€²æ­¥ç¥é€Ÿ"),
            ("æ©Ÿå™¨å­¸ç¿’æ˜¯ AI çš„æ ¸å¿ƒ", "ML æ˜¯äººå·¥æ™ºæ…§çš„é—œéµæŠ€è¡“"),
            ("æ·±åº¦å­¸ç¿’æ”¹è®Šäº†ä¸–ç•Œ", "æ·±åº¦ç¥ç¶“ç¶²çµ¡å¸¶ä¾†é©å‘½æ€§è®ŠåŒ–")
        ]

        # ä¸ç›¸ä¼¼æ–‡æœ¬å°
        dissimilar_pairs = [
            ("äººå·¥æ™ºæ…§æŠ€è¡“ç™¼å±•è¿…é€Ÿ", "ä»Šå¤©å¤©æ°£å¾ˆå¥½"),
            ("æ©Ÿå™¨å­¸ç¿’æ˜¯ AI çš„æ ¸å¿ƒ", "æˆ‘å–œæ­¡åƒæ°´æœ"),
            ("æ·±åº¦å­¸ç¿’æ”¹è®Šäº†ä¸–ç•Œ", "è²“æ˜¯ä¸€ç¨®å¯æ„›çš„å‹•ç‰©")
        ]

        def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
            """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
            import math
            dot_product = sum(a * b for a, b in zip(vec1, vec2))
            magnitude1 = math.sqrt(sum(a * a for a in vec1))
            magnitude2 = math.sqrt(sum(b * b for b in vec2))
            return dot_product / (magnitude1 * magnitude2)

        # æ¸¬è©¦ç›¸ä¼¼æ–‡æœ¬
        similar_scores = []
        for text1, text2 in similar_pairs:
            vec1 = self.embedder.embed(text1)
            vec2 = self.embedder.embed(text2)
            similarity = cosine_similarity(vec1, vec2)
            similar_scores.append(similarity)

        # æ¸¬è©¦ä¸ç›¸ä¼¼æ–‡æœ¬
        dissimilar_scores = []
        for text1, text2 in dissimilar_pairs:
            vec1 = self.embedder.embed(text1)
            vec2 = self.embedder.embed(text2)
            similarity = cosine_similarity(vec1, vec2)
            dissimilar_scores.append(similarity)

        result = {
            "similar_texts": {
                "mean_similarity": round(statistics.mean(similar_scores), 4),
                "min_similarity": round(min(similar_scores), 4),
                "max_similarity": round(max(similar_scores), 4)
            },
            "dissimilar_texts": {
                "mean_similarity": round(statistics.mean(dissimilar_scores), 4),
                "min_similarity": round(min(dissimilar_scores), 4),
                "max_similarity": round(max(dissimilar_scores), 4)
            },
            "separation": round(
                statistics.mean(similar_scores) - statistics.mean(dissimilar_scores), 4
            )
        }

        print(f"  âœ… ç›¸ä¼¼æ–‡æœ¬å¹³å‡ç›¸ä¼¼åº¦: {result['similar_texts']['mean_similarity']}")
        print(f"  âœ… ä¸ç›¸ä¼¼æ–‡æœ¬å¹³å‡ç›¸ä¼¼åº¦: {result['dissimilar_texts']['mean_similarity']}")
        print(f"  âœ… åˆ†é›¢åº¦: {result['separation']}\n")

        return result

    def test_cold_start_time(self) -> Dict[str, Any]:
        """
        æ¸¬è©¦å†·å•Ÿå‹•æ™‚é–“ (æ¨¡å‹è¼‰å…¥æ™‚é–“)

        Returns:
            æ¸¬è©¦çµæœ (æ¨¡å‹è¼‰å…¥æ™‚é–“)
        """
        print("ğŸ“Š æ¸¬è©¦ 4/5: å†·å•Ÿå‹•æ™‚é–“")

        # æ³¨æ„: é€™å€‹æ¸¬è©¦æœƒé‡æ–°è¼‰å…¥æ¨¡å‹ï¼Œå› æ­¤æœƒæ¯”è¼ƒæ…¢
        print("  é‡æ–°è¼‰å…¥æ¨¡å‹ä»¥æ¸¬è©¦å†·å•Ÿå‹•æ™‚é–“...")

        start_time = time.perf_counter()
        config = BaseEmbedderConfig(
            model="BAAI/bge-m3",
            model_kwargs={
                "use_fp16": True,
                "device": "cpu",
                "max_length": 8192
            }
        )
        _ = BGEM3Embedding(config)
        end_time = time.perf_counter()

        cold_start_s = end_time - start_time

        result = {
            "cold_start_time_s": round(cold_start_s, 2),
            "cold_start_time_ms": round(cold_start_s * 1000, 2)
        }

        print(f"  âœ… å†·å•Ÿå‹•æ™‚é–“: {result['cold_start_time_s']}s\n")

        return result

    def test_concurrent_requests(self, num_threads: int = 10) -> Dict[str, Any]:
        """
        æ¸¬è©¦ä¸¦ç™¼è«‹æ±‚æ€§èƒ½

        Args:
            num_threads: ä¸¦ç™¼ç·šç¨‹æ•¸ (é»˜èª 10)

        Returns:
            æ¸¬è©¦çµæœ (ä¸¦ç™¼æ€§èƒ½æŒ‡æ¨™)
        """
        print(f"ğŸ“Š æ¸¬è©¦ 5/5: ä¸¦ç™¼è«‹æ±‚æ€§èƒ½ ({num_threads} ç·šç¨‹)")

        import threading

        test_text = "æ¸¬è©¦ä¸¦ç™¼è«‹æ±‚çš„æ€§èƒ½è¡¨ç¾"
        latencies = []
        errors = []
        lock = threading.Lock()

        def embed_text():
            """å–®å€‹ç·šç¨‹çš„åµŒå…¥ä»»å‹™"""
            try:
                start_time = time.perf_counter()
                _ = self.embedder.embed(test_text)
                end_time = time.perf_counter()
                latency_ms = (end_time - start_time) * 1000

                with lock:
                    latencies.append(latency_ms)
            except Exception as e:
                with lock:
                    errors.append(str(e))

        # å‰µå»ºä¸¦å•Ÿå‹•ç·šç¨‹
        threads = []
        start_time = time.perf_counter()
        for _ in range(num_threads):
            thread = threading.Thread(target=embed_text)
            threads.append(thread)
            thread.start()

        # ç­‰å¾…æ‰€æœ‰ç·šç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        end_time = time.perf_counter()

        total_time_s = end_time - start_time

        result = {
            "num_threads": num_threads,
            "total_time_s": round(total_time_s, 2),
            "throughput_requests_per_s": round(num_threads / total_time_s, 2),
            "mean_latency_ms": round(statistics.mean(latencies), 2) if latencies else None,
            "p95_latency_ms": round(sorted(latencies)[int(len(latencies) * 0.95)], 2) if latencies else None,
            "errors": len(errors),
            "success_rate": round((len(latencies) / num_threads) * 100, 2)
        }

        print(f"  âœ… ååé‡: {result['throughput_requests_per_s']} req/s")
        print(f"  âœ… å¹³å‡å»¶é²: {result['mean_latency_ms']}ms")
        print(f"  âœ… æˆåŠŸç‡: {result['success_rate']}%\n")

        return result

    def run_all_tests(self) -> Dict[str, Any]:
        """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
        print("=" * 60)
        print("ğŸš€ Mem0Evomem æ€§èƒ½åŸºæº–æ¸¬è©¦")
        print("=" * 60)
        print()

        # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
        self.results["tests"]["single_text_latency"] = self.test_single_text_embedding_latency()
        self.results["tests"]["batch_throughput"] = self.test_batch_embedding_throughput()
        self.results["tests"]["embedding_quality"] = self.test_embedding_quality()
        self.results["tests"]["cold_start"] = self.test_cold_start_time()
        self.results["tests"]["concurrent_requests"] = self.test_concurrent_requests()

        return self.results

    def generate_report(self, output_file: str = "data/benchmarks/performance_report.json"):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        import os

        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        # ä¿å­˜ JSON å ±å‘Š
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)

        print("=" * 60)
        print("ğŸ“Š æ¸¬è©¦å®Œæˆï¼")
        print("=" * 60)
        print()
        print(f"âœ… å ±å‘Šå·²ç”Ÿæˆ: {output_file}")
        print()

        # æ‰“å°æ‘˜è¦
        print("ğŸ“ˆ æ€§èƒ½æ‘˜è¦:")
        print(f"  - å–®æ–‡æœ¬åµŒå…¥ P50: {self.results['tests']['single_text_latency']['p50_ms']}ms")
        print(f"  - å–®æ–‡æœ¬åµŒå…¥ P95: {self.results['tests']['single_text_latency']['p95_ms']}ms")
        print(f"  - æ‰¹æ¬¡ååé‡ (100): {self.results['tests']['batch_throughput']['batch_100']['throughput_texts_per_s']} texts/s")
        print(f"  - å†·å•Ÿå‹•æ™‚é–“: {self.results['tests']['cold_start']['cold_start_time_s']}s")
        print(f"  - ä¸¦ç™¼ååé‡ (10 ç·šç¨‹): {self.results['tests']['concurrent_requests']['throughput_requests_per_s']} req/s")
        print(f"  - èªç¾©åˆ†é›¢åº¦: {self.results['tests']['embedding_quality']['separation']}")
        print()


def main():
    """ä¸»å‡½æ•¸"""
    benchmark = PerformanceBenchmark()
    benchmark.run_all_tests()
    benchmark.generate_report()


if __name__ == "__main__":
    main()
